<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
     
    <title>Wind&#39;s Howling  | Learning Topic-Sensitive Word Representations</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
     <meta name="generator" content="Hugo 0.21-DEV" />
      
      
        <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
      

     <link href='http://kevinkwl.me/assets/css/style.css' rel='stylesheet' type="text/css" />
     <link href='http://kevinkwl.me/assets/css/prism.css' rel='stylesheet' type="text/css" />  
     
     

     

      <meta property="og:title" content="Learning Topic-Sensitive Word Representations" />
<meta property="og:description" content="Some notes on the ACL2017 paper: _Learning Topic-Sensitive Word Representations_." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://kevinkwl.me/writing/learn-topic-sensitive-word-representations/" />



<meta property="article:published_time" content="2017-05-08T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2017-05-08T00:00:00&#43;00:00"/>











<meta itemprop="name" content="Learning Topic-Sensitive Word Representations">
<meta itemprop="description" content="Some notes on the ACL2017 paper: _Learning Topic-Sensitive Word Representations_.">


<meta itemprop="dateModified" content="2017-05-08T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="386">



<meta itemprop="keywords" content="misc,notes,ramblings,algorithm,fibonacci,life,linux,nlp,orgmode,writing," />


  <meta name="twitter:card" content="summary"/>



<meta name="twitter:text:title" content="Learning Topic-Sensitive Word Representations"/>
<meta name="twitter:title" content="Learning Topic-Sensitive Word Representations"/>
<meta name="twitter:description" content="Some notes on the ACL2017 paper: _Learning Topic-Sensitive Word Representations_."/>




  </head>

  <body class="">
    <div id="nav-wrapper">
<nav class="navbar" role="navigation">
  <div class="left">
    <a href="http://kevinkwl.me/">Wind&#39;s Howling</a>
  </div>

  <div class="right">
    <ul>
      <li>
        <a class="link" href="http://kevinkwl.me/writing">Writings</a>
      </li>
      <li>
        <a class="link" href="http://kevinkwl.me/tags">Tags</a>
      </li>
      <li>
        <a class="link" href="http://kevinkwl.me/about">About</a>
      </li>
    </ul>
  </div>
</nav>
</div>

    

    <main role="main" >
      
<div class="post-wrapper">
  <div class="container">
    <article>
      <div class="post-meta">
        <span class="post-category">
          
            <a class="category" href="http://kevinkwl.me/category/notes/">Notes</a>
          
        </span>

      <h1 class="title">
        Learning Topic-Sensitive Word Representations
      </h1>

        <span class="post-date">
          8 May, 2017
        </span>
        &bullet;
        <span class="read-time">
          2 min read
        </span>
      </div>
      
      <section class="content">
        <p><a href="https://arxiv.org/pdf/1705.00441.pdf" title="Link to paper">Link to paper</a></p>

<h1 id="about-word-embedding">About Word Embedding</h1>

<p>The concept (or technique) of <strong>Word Embedding</strong> plays a crucial role in the
recent Natural Language Processing field. Instead of high-dimensional
(vocabulary size), extremely sparse (one-hot) vectors, smaller
size (< 1k dimensions) real-valued vectors are used to model words and languages.
These smaller, dense real-valued vectors are referred to distributed word
representation, in that the information about the word is distributed among each
dimension.</p>

<p>Most existing word embedding models (word2vec, glove, ...) only use one
representation per word. Though these models have already achieved amazing
results on many task, they fail to distinguish different meanings of polysemous
words. For example, the word <code>bank</code>, has two distinct meanings which can only be
inferred from the context. However, the two meanings share the same word
representation.</p>

<h1 id="topic-sensitive-embedding">Topic-Sensitive Embedding</h1>

<p>There are many existing models that deal with these problem, but the author points out
that these methods are laborious and limited to few languages. Also, word senses
are difficult to capture and define. In this paper, the <strong>Hierarchicial Dirichlet
Process</strong> topic model is introduced and embedded into a skip-gram model.</p>

<p>Each document can have several topics, and each word in that document is
associated with one topic. In the case of learning word representations, the
document can be a sentence or a group of sentences that come from the same
document. It's assumed that the sense of a word is related to the topic. So it
is actually topic-word embeddings.</p>

<p>HDP is used to learn all topics from a subset of the corpus, then run HDP on the
same corpus to obtain all word-topic labeling and document-level topics
distribution. These two distribution, along with the context of words, are used
in Skip-gram learning.</p>

<p>As a modification to skip-gram, the input word (center-word) is a word-topic
pair, and for output word (context words), normal generic representations are
used, in order to reduce sparsity.</p>

<h1 id="models">Models</h1>

<p>Three approaches are proposed, they are:</p>

<ul>
<li>HTLE, hard-topic labeled representation. In this setting, each word-topic pair</li>
  is a separate vocabulary entry (for input word, aka target word), and unlabeled word
  representations for context words (output).
</ul>

<p>$$\textbf{h}_{HTLE}(w_i) = \textbf{r}(w_i^\tau)$$</p>

<ul>
<li>HTLEadd, modify the target word embedding to be the sum of word-topic</li>
  embedding and a generic word embedding. In this way, the training process
  shares some information between different topic-word representations for the
  same word.
</ul>

<p>$$\textbf{h}_{HTLEadd}(w_i) = \textbf{r'}(w_i^\tau) + {\bf r_o}(w_i)$$</p>

<ul>
<li>STLE, soft-topic labeled representations. For each update, use the topic</li>
  distribution to compute a weighted sum over the word-topic embedding
</ul>

<p>$$\textbf{h}_{STLE}(w_i) = \sum_{k=1}^Tp(\tau_k | d_i) \textbf{r''}(w_i^{\tau_k})$$</p>

<p><img src="/assets/img/topic-sensitive-embedding/models.png" alt="/assets/img/topic-sensitive-embedding/models.png" title="/assets/img/topic-sensitive-embedding/models.png" /></p>

      </section>
      <div class="after-post-meta">
      <ul class="all-tags">
        
        <li>
        <a class="tag" href="http://kevinkwl.me/tags/nlp/">NLP</a>
        </li>
        
      </div>
      </div>
    </article>
    <div class="ph3 mt2 mt6-ns">
      <div class="related">
  <h4 class="relatedPost">You may also like...</h4>
  <ul class="relatedPosts">
    
  </ul>
</div>

    </div>
    <div class="disqus">
  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
        this.page.url = "http:\/\/kevinkwl.me\/writing\/learn-topic-sensitive-word-representations\/";
        this.page.identifier = "http:\/\/kevinkwl.me\/writing\/learn-topic-sensitive-word-representations\/";
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = '//winds-howling.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>

  </div>
</div>

    </main>
    <footer class="bg-white boxShadowTop" role="contentinfo">
  <div class="footer-social">
  <div class="social">
  <a class="icon-wrapper" href="https://github.com/kevinkwl">
    <div>
      <svg class="social" viewBox="0 0 64 64">
        <use class="use-icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/assets/svg/social-accounts.svg#github-icon"></use>
        <use class="use-mask" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/assets/svg/social-accounts.svg#github-mask"></use>
      </svg>
    </div>
  </a>
  <a class="icon-wrapper" href="https://linkedin.com/in/kangwei-ling-520aab131">
    <div>
      <svg class="social" viewBox="0 0 64 64">
        <use class="use-icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/assets/svg/social-accounts.svg#linkedin-icon"></use>
        <use class="use-mask" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/assets/svg/social-accounts.svg#linkedin-mask"></use>
      </svg>
    </div>
  </a>
  <a class="icon-wrapper" href="mailto:lingkangwei.kevin@gmail.com">
    <div>
      <svg class="social" viewBox="0 0 64 64">
        <use class="use-icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/assets/svg/social-accounts.svg#email-icon"></use>
        <use class="use-mask" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/assets/svg/social-accounts.svg#email-mask"></use>
      </svg>
    </div>
  </a>
</div>

  </div>
  <div class="copyright">
  <span>
    Copyright &copy; 2017 Kevin Ling, kevinkwl
  </span>
  </div>
</footer>

    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>


<script src="/assets/js/prism.js"></script>



<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-98612858-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


  </body>
</html>
